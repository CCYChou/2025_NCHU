# single_stage_optimization.py - å–®éšæ®µå„ªåŒ–

import numpy as np
import pickle
import torch
import torch.nn as nn
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import train_test_split
import seaborn as sns
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.utils import set_random_seed
from gymnasium import Env
from gymnasium.spaces import Discrete, Box
from collections import deque
import warnings
warnings.filterwarnings('ignore')

# ä½¿ç”¨ç³»çµ±é»˜èªå­—é«”
import matplotlib
matplotlib.rcParams.update(matplotlib.rcParamsDefault)
plt.rcParams['font.size'] = 10
plt.rcParams['axes.titlesize'] = 12
plt.rcParams['axes.labelsize'] = 10
plt.rcParams['legend.fontsize'] = 9

set_random_seed(42)
torch.manual_seed(42)
np.random.seed(42)

class SingleStageWaferEnv(Env):
    """å–®éšæ®µå„ªåŒ–ç’°å¢ƒ """
    def __init__(self, wafer_data, labels, num_classes):
        super().__init__()
        self.wafer_data = wafer_data
        self.labels = labels
        self.num_classes = num_classes
        self.episode_length = 60  # å›åˆ°è¼ƒçŸ­çš„episodeä»¥æé«˜è¨“ç·´æ•ˆç‡
        
        # åˆ†æé¡åˆ¥åˆ†å¸ƒï¼ˆåŸºæ–¼æ‚¨çš„æ··æ·†çŸ©é™£ï¼‰
        unique, counts = np.unique(labels, return_counts=True)
        self.class_info = {}
        total = len(labels)
        
        for cls, count in zip(unique, counts):
            frequency = count / total
            # åŸºæ–¼æ‚¨çš„çµæœï¼Œè­˜åˆ¥å›°é›£é¡åˆ¥
            if cls in [0, 2, 4, 5, 6, 7]:  # å¾æ··æ·†çŸ©é™£çœ‹å‡ºçš„å›°é›£é¡åˆ¥
                difficulty = 2.5
            elif cls in [1]:  # ä¸­ç­‰é›£åº¦
                difficulty = 1.8
            else:  # cls in [3, 8] - ç›¸å°å®¹æ˜“çš„é¡åˆ¥
                difficulty = 1.0
            
            self.class_info[cls] = {
                'count': count,
                'frequency': frequency,
                'difficulty': difficulty
            }
        
        print(f"ğŸ“Š é¡åˆ¥åˆ†æå®Œæˆ")
        print(f"å›°é›£é¡åˆ¥ [0,2,4,5,6,7]: {sum(counts[i] for i in [0,2,4,5,6,7] if i < len(counts))} æ¨£æœ¬")
        print(f"å®¹æ˜“é¡åˆ¥ [3,8]: {sum(counts[i] for i in [3,8] if i < len(counts))} æ¨£æœ¬")
        
        # æå–ç‰¹å¾µ 
        self.features = self._extract_effective_features(wafer_data)
        
        # å®šç¾©ç©ºé–“
        feature_dim = self.features.shape[1]
        context_dim = 8
        self.observation_space = Box(
            low=-np.inf, high=np.inf, 
            shape=(feature_dim + context_dim,), dtype=np.float32
        )
        self.action_space = Discrete(num_classes)
        
        # æ€§èƒ½è¿½è¹¤
        self.class_performance = {i: deque(maxlen=20) for i in range(num_classes)}
        self.confusion_memory = np.zeros((num_classes, num_classes))
        
        # åˆå§‹åŒ–
        self.current_step = 0
        self.correct_count = 0
        self.current_idx = 0
        
    def _extract_effective_features(self, wafer_data):
        """æå–æœ‰æ•ˆç‰¹å¾µ """
        print("ğŸ”§ æå–æœ‰æ•ˆç‰¹å¾µ...")
        features = []
        
        for i, wafer in enumerate(wafer_data):
            if i % 1000 == 0:
                print(f"  è™•ç† {i}/{len(wafer_data)}")
            
            # æ­£è¦åŒ–
            wafer_norm = wafer.astype(np.float32) / 255.0 if wafer.max() > 1 else wafer.astype(np.float32)
            
            # 1. åŸºç¤ç‰¹å¾µï¼ˆæœ€é‡è¦ï¼‰
            flat = wafer_norm.flatten()
            
            # 2. æ ¸å¿ƒçµ±è¨ˆç‰¹å¾µ
            stats = [
                np.mean(wafer_norm), np.std(wafer_norm), np.median(wafer_norm),
                np.min(wafer_norm), np.max(wafer_norm), 
                np.percentile(wafer_norm, 25), np.percentile(wafer_norm, 75),
                np.sum(wafer_norm > np.mean(wafer_norm)) / wafer_norm.size
            ]
            
            # 3. é—œéµç©ºé–“ç‰¹å¾µ
            h, w = wafer_norm.shape
            center_h, center_w = h//2, w//2
            
            # åªè¨ˆç®—æœ€æœ‰æ•ˆçš„å€åŸŸç‰¹å¾µ
            regions = [
                wafer_norm[:center_h, :center_w],      # å·¦ä¸Š
                wafer_norm[center_h:, center_w:]       # å³ä¸‹
            ]
            
            region_stats = []
            for region in regions:
                region_stats.extend([
                    np.mean(region), np.std(region),
                    np.sum(region > np.mean(wafer_norm)) / region.size
                ])
            
            # 4. ç°¡å–®é‚Šç·£ç‰¹å¾µ
            if h > 1 and w > 1:
                edge_h = np.mean(np.abs(np.diff(wafer_norm, axis=0)))
                edge_w = np.mean(np.abs(np.diff(wafer_norm, axis=1)))
                edge_features = [edge_h, edge_w]
            else:
                edge_features = [0, 0]
            
            # çµ„åˆç‰¹å¾µ
            combined = np.concatenate([flat, stats, region_stats, edge_features])
            features.append(combined)
        
        features = np.array(features)
        print(f"âœ… ç‰¹å¾µæå–å®Œæˆï¼Œç¶­åº¦: {features.shape}")
        return features
    
    def reset(self, seed=None, options=None):
        self.current_step = 0
        self.correct_count = 0
        self.current_idx = np.random.randint(0, len(self.features))
        
        obs = self._get_observation()
        return obs, {}
    
    def _get_observation(self):
        feature = self.features[self.current_idx].copy()
        
        # ç²¾ç°¡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        step_progress = self.current_step / self.episode_length
        accuracy_so_far = self.correct_count / max(1, self.current_step)
        true_label = self.labels[self.current_idx]
        
        # é¡åˆ¥ç›¸é—œä¿¡æ¯
        difficulty = self.class_info[true_label]['difficulty']
        frequency = self.class_info[true_label]['frequency']
        
        # æ­·å²è¡¨ç¾
        class_perf = np.mean(self.class_performance[true_label]) if self.class_performance[true_label] else 0.5
        
        # ä¸Šä¸‹æ–‡
        context = np.array([
            step_progress,
            accuracy_so_far,
            float(true_label),
            difficulty,
            frequency,
            class_perf,
            float(step_progress < 0.3),  # æ—©æœŸ
            float(step_progress > 0.7)   # å¾ŒæœŸ
        ], dtype=np.float32)
        
        obs = np.concatenate([feature, context])
        return obs
    
    def step(self, action):
        true_label = self.labels[self.current_idx]
        is_correct = (action == true_label)
        
        # æ›´æ–°æ··æ·†è¨˜æ†¶
        self.confusion_memory[true_label, action] += 1
        
        # è¨ˆç®—çå‹µ
        reward = self._calculate_focused_reward(action, true_label, is_correct)
        
        # æ›´æ–°çµ±è¨ˆ
        if is_correct:
            self.correct_count += 1
        
        self.class_performance[true_label].append(float(is_correct))
        
        self.current_step += 1
        done = self.current_step >= self.episode_length
        
        # EpisodeçµæŸè™•ç†
        if done:
            episode_accuracy = self.correct_count / self.episode_length
            episode_reward = self._calculate_episode_reward(episode_accuracy)
            reward += episode_reward
        
        # æ™ºèƒ½æ¡æ¨£
        if not done:
            self.current_idx = self._focused_sampling()
        
        next_obs = self._get_observation()
        
        info = {
            'accuracy': self.correct_count / max(1, self.current_step),
            'episode_accuracy': self.correct_count / self.episode_length if done else None
        }
        
        return next_obs, reward, done, False, info
    
    def _calculate_focused_reward(self, action, true_label, is_correct):
        """å°ˆæ³¨çš„çå‹µå‡½æ•¸ - é‡å°75%â†’80%çš„æå‡"""
        if is_correct:
            base_reward = 15.0  # æé«˜åŸºç¤çå‹µ
            
            # å›°é›£é¡åˆ¥å¤§å¹…çå‹µ
            difficulty = self.class_info[true_label]['difficulty']
            base_reward *= difficulty
            
            # ç•¶å‰è¡¨ç¾çå‹µ
            current_acc = self.correct_count / max(1, self.current_step)
            if current_acc > 0.8:
                base_reward += 15.0  # å¤§å¹…çå‹µé«˜æ€§èƒ½
            elif current_acc > 0.75:
                base_reward += 8.0
            
            # é¡åˆ¥æ”¹é€²çå‹µ
            if self.class_performance[true_label]:
                recent_perf = np.mean(list(self.class_performance[true_label])[-5:])
                if recent_perf > 0.8:
                    base_reward += 10.0
            
            return base_reward
        
        else:
            base_penalty = -5.0
            
            # ç°¡å–®é¡åˆ¥éŒ¯èª¤åš´é‡æ‡²ç½°
            if true_label in [3, 8]:  # æ‡‰è©²å®¹æ˜“åˆ†é¡çš„é¡åˆ¥
                base_penalty *= 2.0
            
            # å¸¸è¦‹éŒ¯èª¤æ‡²ç½°
            if self.confusion_memory[true_label, action] > 10:
                base_penalty *= 1.5
            
            return base_penalty
    
    def _calculate_episode_reward(self, episode_accuracy):
        """EpisodeçµæŸçå‹µ - é‡å°80%ç›®æ¨™"""
        if episode_accuracy >= 0.85:
            return 200.0  # è¶…è¶Šç›®æ¨™çš„å¤§çå‹µ
        elif episode_accuracy >= 0.8:
            return 150.0  # é”åˆ°ç›®æ¨™çš„å¤§çå‹µ
        elif episode_accuracy >= 0.78:
            return 100.0  # æ¥è¿‘ç›®æ¨™
        elif episode_accuracy >= 0.75:
            return 50.0   # ä¿æŒ75%æ°´å¹³
        elif episode_accuracy < 0.7:
            return -50.0  # ä½æ–¼75%çš„æ‡²ç½°
        else:
            return 0.0
    
    def _focused_sampling(self):
        """å°ˆæ³¨æ¡æ¨£ - é‡é»é—œæ³¨å›°é›£é¡åˆ¥"""
        # 50%æ©Ÿç‡é¸æ“‡å›°é›£é¡åˆ¥
        if np.random.random() < 0.5:
            # è­˜åˆ¥è¡¨ç¾æœ€å·®çš„é¡åˆ¥
            poor_classes = []
            for cls, perfs in self.class_performance.items():
                if len(perfs) >= 3 and np.mean(perfs) < 0.7:
                    poor_classes.append(cls)
            
            if poor_classes:
                target_class = np.random.choice(poor_classes)
                class_indices = np.where(self.labels == target_class)[0]
                return np.random.choice(class_indices)
        
        return np.random.randint(0, len(self.features))

class FocusedCallback(BaseCallback):
    """å°ˆæ³¨å›èª¿ - ç›£æ§80%ç›®æ¨™é€²åº¦"""
    def __init__(self, eval_freq=2000, target_accuracy=0.8):
        super().__init__()
        self.eval_freq = eval_freq
        self.target_accuracy = target_accuracy
        self.best_reward = -np.inf
        self.accuracy_estimates = []
        self.consecutive_improvements = 0
        
    def _on_step(self) -> bool:
        if self.num_timesteps % self.eval_freq == 0:
            if len(self.model.ep_info_buffer) > 0:
                mean_reward = np.mean([ep_info['r'] for ep_info in self.model.ep_info_buffer])
                
                # ä¼°ç®—æº–ç¢ºç‡
                estimated_acc = min(0.9, max(0.6, (mean_reward + 50) / 250))
                self.accuracy_estimates.append(estimated_acc)
                
                if mean_reward > self.best_reward:
                    self.best_reward = mean_reward
                    self.consecutive_improvements += 1
                    
                    print(f"ğŸ¯ æ”¹é€² #{self.consecutive_improvements}: çå‹µ={mean_reward:.1f}, ä¼°ç®—æº–ç¢ºç‡={estimated_acc:.3f}")
                    
                    # å¦‚æœä¼°ç®—æº–ç¢ºç‡æ¥è¿‘80%ï¼Œä¿å­˜æ¨¡å‹
                    if estimated_acc >= 0.78:
                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                        self.model.save(f'focused_high_perf_{timestamp}')
                        print(f"ğŸ’¾ é«˜æ€§èƒ½æ¨¡å‹å·²ä¿å­˜")
                else:
                    self.consecutive_improvements = 0
                
                print(f"æ­¥æ•¸: {self.num_timesteps:,}, çå‹µ: {mean_reward:.1f}, è¶¨å‹¢: {'â†‘' if len(self.accuracy_estimates) >= 2 and self.accuracy_estimates[-1] > self.accuracy_estimates[-2] else 'â†“'}")
        
        return True

def load_data():
    print("ğŸ“ è¼‰å…¥è³‡æ–™...")
    try:
        with open('./data/x_train_org_20210614.pickle','rb') as f:
            x_train = pickle.load(f)
        with open('./data/y_train_org_20210614.pickle','rb') as f:
            y_train = pickle.load(f)
        with open('./data/x_test_20210614.pickle','rb') as f:
            x_test = pickle.load(f)
        with open('./data/y_test_20210614.pickle','rb') as f:
            y_test = pickle.load(f)
        
        return np.array(x_train), np.array(y_train).ravel(), np.array(x_test), np.array(y_test).ravel()
    except Exception as e:
        print(f"è¼‰å…¥å¤±æ•—: {e}")
        return None, None, None, None

def main():
    print("ğŸš€ å–®éšæ®µå„ªåŒ–PPO - å°ˆæ³¨75%â†’80%æå‡")
    
    # è¼‰å…¥æ•¸æ“š
    x_train, y_train, x_test, y_test = load_data()
    if x_train is None:
        print("âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—")
        return
    
    # æº–å‚™æ•¸æ“š
    le = LabelEncoder()
    y_train_enc = le.fit_transform(y_train)
    y_test_enc = le.transform(y_test)
    num_classes = len(le.classes_)
    
    print(f"é¡åˆ¥æ•¸é‡: {num_classes}")
    print(f"è¨“ç·´æ¨£æœ¬: {len(x_train)}, æ¸¬è©¦æ¨£æœ¬: {len(x_test)}")
    
    # ä¿å®ˆçš„æ•¸æ“šå¹³è¡¡
    print("âš–ï¸ ä¿å®ˆæ•¸æ“šå¹³è¡¡...")
    unique_labels, counts = np.unique(y_train_enc, return_counts=True)
    
    balanced_x, balanced_y = [], []
    target_count = min(200, max(counts))  # ä¿å®ˆçš„ç›®æ¨™æ•¸é‡
    
    for label in unique_labels:
        label_indices = np.where(y_train_enc == label)[0]
        current_count = len(label_indices)
        
        if current_count < 80:  # æœ€å°‘80å€‹æ¨£æœ¬
            sampled_indices = np.random.choice(label_indices, 80, replace=True)
        elif current_count > target_count:
            sampled_indices = np.random.choice(label_indices, target_count, replace=False)
        else:
            sampled_indices = label_indices
        
        balanced_x.extend(x_train[sampled_indices])
        balanced_y.extend([label] * len(sampled_indices))
    
    X_train_balanced = np.array(balanced_x)
    y_train_balanced = np.array(balanced_y)
    
    # è¨“ç·´é›†æº–å‚™
    X_train_final, _, y_train_final, _ = train_test_split(
        X_train_balanced, y_train_balanced, test_size=0.1,  # åªç•™å°‘é‡é©—è­‰
        random_state=42, stratify=y_train_balanced
    )
    
    print(f"æœ€çµ‚è¨“ç·´æ¨£æœ¬: {len(X_train_final)}")
    
    # å‰µå»ºç’°å¢ƒ
    print("ğŸŒ å»ºç«‹å°ˆæ³¨ç’°å¢ƒ...")
    def make_env():
        return SingleStageWaferEnv(X_train_final, y_train_final, num_classes)
    
    env = DummyVecEnv([make_env])
    callback = FocusedCallback()
    
    # å„ªåŒ–çš„PPOé…ç½®
    print("ğŸ§  å»ºç«‹å°ˆæ³¨PPO...")
    model = PPO(
        'MlpPolicy',
        env,
        verbose=1,
        learning_rate=1.5e-4,    # è¼ƒä½çš„å­¸ç¿’ç‡ä¿è­‰ç©©å®šæ€§
        n_steps=768,             # é©ä¸­çš„æ­¥æ•¸
        batch_size=96,           # é©ä¸­çš„æ‰¹é‡
        n_epochs=15,             # æ›´å¤šçš„è¨“ç·´è¼ªæ•¸
        gamma=0.98,              # ç¨å¾®é™ä½æŠ˜æ‰£å› å­
        gae_lambda=0.95,
        clip_range=0.18,         # é©ä¸­çš„clipç¯„åœ
        ent_coef=0.025,          # å¢åŠ æ¢ç´¢
        vf_coef=0.5,
        max_grad_norm=0.5,
        policy_kwargs={
            'net_arch': dict(pi=[384, 192, 96], vf=[384, 192, 96]),
        }
    )
    
    print("ğŸš€ é–‹å§‹å°ˆæ³¨è¨“ç·´...")
    print("ğŸ¯ ç›®æ¨™: ç©©å®šå¾75%æå‡åˆ°80%")
    
    # è¨“ç·´
    try:
        model.learn(total_timesteps=100000, callback=callback, progress_bar=True)
    except KeyboardInterrupt:
        print("â¹ï¸ è¨“ç·´ä¸­æ–·")
    
    # æœ€çµ‚è©•ä¼°
    print("\nğŸ“Š æœ€çµ‚è©•ä¼°...")
    test_env = SingleStageWaferEnv(x_test, y_test_enc, num_classes)
    
    test_predictions = []
    for i in range(len(x_test)):
        test_env.current_idx = i
        obs = test_env._get_observation()
        action, _ = model.predict(obs, deterministic=True)
        test_predictions.append(action)
    
    final_accuracy = accuracy_score(y_test_enc, test_predictions)
    
    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    model.save(f'single_stage_final_{timestamp}')
    
    print(f"\nğŸ¯ æœ€çµ‚çµæœ:")
    print(f"  æ¸¬è©¦æº–ç¢ºç‡: {final_accuracy:.4f}")
    print(f"  æ”¹é€²æ¬¡æ•¸: {callback.consecutive_improvements}")
    print(f"  vs 74.9%: {(final_accuracy - 0.749):.4f}")
    
    # æˆåŠŸåˆ¤æ–·
    if final_accuracy >= 0.8:
        print("ğŸ‰ æˆåŠŸé”åˆ°80%ç›®æ¨™ï¼")
    elif final_accuracy >= 0.77:
        print("ğŸ”¥ å¾ˆæ¥è¿‘ç›®æ¨™ï¼Œå»ºè­°ç¹¼çºŒè¨“ç·´")
    elif final_accuracy >= 0.75:
        print("âœ… ä¿æŒäº†75%æ°´å¹³")
    else:
        print("âš ï¸ éœ€è¦æª¢æŸ¥é…ç½®")
    
    # ç°¡å–®å¯è¦–åŒ–ï¼ˆé¿å…å­—é«”å•é¡Œï¼‰
    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
    
    # é€²åº¦æ¢å½¢åœ–
    milestones = ['Previous Best', 'Current Result', 'Target']
    values = [0.749, final_accuracy, 0.8]
    colors = ['orange', 'blue' if final_accuracy >= 0.8 else 'red', 'green']
    
    bars = ax.bar(milestones, values, color=colors, alpha=0.7, edgecolor='black')
    ax.set_title('Progress Toward 80% Target')
    ax.set_ylabel('Accuracy')
    ax.set_ylim(0.7, 0.85)
    ax.grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ•¸å€¼
    for bar, val in zip(bars, values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                f'{val:.3f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(f'single_stage_result_{timestamp}.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # è©³ç´°åˆ†æ
    print(f"\nğŸ“Š è©³ç´°åˆ†æ:")
    print(classification_report(y_test_enc, test_predictions, zero_division=0))

if __name__ == '__main__':
    main()
