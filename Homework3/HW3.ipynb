# %% [markdown]
# HW3 – Multi-Armed Bandit Exploration vs Exploitation
#
# This Colab-ready notebook script implements four Bernoulli bandit algorithms.
# For each algorithm:
# 1. LaTeX Formula
# 2. ChatGPT Prompt
# 3. Python Code & Plot
# 4. Result Interpretation

# %% [markdown]
# **Setup**
# Install dependencies (uncomment if needed in Colab)
# !pip install numpy matplotlib

# %%
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

# Simulation parameters
tmp_seed = 42
np.random.seed(tmp_seed)
true_probs = [0.1, 0.5, 0.8, 0.3, 0.6]
HORIZON = 1000
RUNS = 100

# %% [markdown]
# ## 1. Epsilon-Greedy
# **LaTeX Formula**
# $$
# \pi_t(a)=
# \begin{cases}
# \arg\max_{a'} Q_t(a'), &\text{w.p. }1-\epsilon\\
# \text{random action}, &\text{w.p. }\epsilon
# \end{cases}
# $$
# **ChatGPT Prompt:**
# > Explain the Epsilon-Greedy algorithm’s exploration–exploitation trade-off and how parameter $\epsilon$ affects long-term reward.

# %%
def epsilon_greedy(probs, horizon, epsilon=0.1):
    Q = np.zeros(len(probs))
    N = np.zeros(len(probs))
    rewards = np.zeros(horizon)
    for t in range(horizon):
        if np.random.rand() < epsilon:
            a = np.random.randint(len(probs))
        else:
            a = np.argmax(Q)
        r = float(np.random.rand() < probs[a])
        N[a] += 1
        Q[a] += (r - Q[a]) / N[a]
        rewards[t] = r
    return np.cumsum(rewards)

# %%
# Run and plot Epsilon-Greedy
cumul_eg = np.zeros(HORIZON)
for _ in range(RUNS):
    cumul_eg += epsilon_greedy(true_probs, HORIZON, epsilon=0.1)
cumul_eg /= RUNS

plt.figure(figsize=(6,4))
plt.plot(cumul_eg, label='Epsilon-Greedy (ε=0.1)')
plt.xlabel('Step')
plt.ylabel('Cumulative Reward')
plt.title('Epsilon-Greedy')
plt.legend()
plt.grid()
plt.show()

# %% [markdown]
# **Interpretation:**
# - Initial fluctuations due to exploration; later stable exploitation of best arm.
# - Higher ε increases exploration but slows convergence; lower ε risks local optima.

# %% [markdown]
# ## 2. UCB1
# **LaTeX Formula**
# $$
# a_t = \arg\max_a \Bigl(Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}\Bigr)
# $$
# **ChatGPT Prompt:**
# > Describe how UCB1 balances exploration and exploitation using its confidence term, and the practical role of parameter $c$.

# %%
def ucb1(probs, horizon, c=2.0):
    Q = np.zeros(len(probs))
    N = np.zeros(len(probs))
    rewards = np.zeros(horizon)
    # Initialize each arm once
    for a in range(len(probs)):
        r = float(np.random.rand() < probs[a])
        Q[a] = r
        N[a] = 1
        rewards[a] = r
    for t in range(len(probs), horizon):
        ucb = Q + c * np.sqrt(np.log(t+1) / N)
        a = np.argmax(ucb)
        r = float(np.random.rand() < probs[a])
        N[a] += 1
        Q[a] += (r - Q[a]) / N[a]
        rewards[t] = r
    return np.cumsum(rewards)

# %%
# Run and plot UCB1
cumul_ucb = np.zeros(HORIZON)
for _ in range(RUNS):
    cumul_ucb += ucb1(true_probs, HORIZON, c=2.0)
cumul_ucb /= RUNS

plt.figure(figsize=(6,4))
plt.plot(cumul_ucb, label='UCB1 (c=2)')
plt.xlabel('Step')
plt.ylabel('Cumulative Reward')
plt.title('UCB1')
plt.legend()
plt.grid()
plt.show()

# %% [markdown]
# **Interpretation:**
# - Confidence term guides exploration; focuses on optimal arm long-term.
# - Large c over-explores; small c under-explores.

# %% [markdown]
# ## 3. Softmax (Boltzmann)
# **LaTeX Formula**
# $$
# P_t(a)=\frac{e^{Q_t(a)/\tau}}{\sum_b e^{Q_t(b)/\tau}}
# $$
# **ChatGPT Prompt:**
# > Explain Softmax action selection and how temperature $\tau$ controls exploration vs exploitation.

# %%
def softmax_selection(probs, horizon, tau=0.1):
    Q = np.zeros(len(probs))
    N = np.zeros(len(probs))
    rewards = np.zeros(horizon)
    for t in range(horizon):
        exp_q = np.exp(Q / tau)
        p = exp_q / exp_q.sum()
        a = np.random.choice(len(probs), p=p)
        r = float(np.random.rand() < probs[a])
        N[a] += 1
        Q[a] += (r - Q[a]) / N[a]
        rewards[t] = r
    return np.cumsum(rewards)

# %%
# Run and plot Softmax
cumul_sm = np.zeros(HORIZON)
for _ in range(RUNS):
    cumul_sm += softmax_selection(true_probs, HORIZON, tau=0.1)
cumul_sm /= RUNS

plt.figure(figsize=(6,4))
plt.plot(cumul_sm, label='Softmax (τ=0.1)')
plt.xlabel('Step')
plt.ylabel('Cumulative Reward')
plt.title('Softmax')
plt.legend()
plt.grid()
plt.show()

# %% [markdown]
# **Interpretation:**
# - Probability-weighted exploration; low τ converges fast but risks local optima; high τ maintains exploration.

# %% [markdown]
# ## 4. Thompson Sampling
# **LaTeX Formula**
# $$
# \theta_a\sim\mathrm{Beta}(\alpha_a,\beta_a),\quad a_t=\arg\max_a\theta_a
# $$
# **ChatGPT Prompt:**
# > Outline Thompson Sampling using Beta priors for Bernoulli bandits and its adaptive exploration.

# %%
def thompson_sampling(probs, horizon):
    alpha = np.ones(len(probs))
    beta_params = np.ones(len(probs))
    rewards = np.zeros(horizon)
    for t in range(horizon):
        samples = np.random.beta(alpha, beta_params)
        a = int(np.argmax(samples))
        r = float(np.random.rand() < probs[a])
        alpha[a] += r
        beta_params[a] += (1 - r)
        rewards[t] = r
    return np.cumsum(rewards)

# %%
# Run and plot Thompson Sampling
cumul_ts = np.zeros(HORIZON)
for _ in range(RUNS):
    cumul_ts += thompson_sampling(true_probs, HORIZON)
cumul_ts /= RUNS

plt.figure(figsize=(6,4))
plt.plot(cumul_ts, label='Thompson Sampling')
plt.xlabel('Step')
plt.ylabel('Cumulative Reward')
plt.title('Thompson Sampling')
plt.legend()
plt.grid()
plt.show()

# %% [markdown]
# **Interpretation:**
# - Adaptive Bayesian exploration; quickly focuses on optimal arm.

# %% [markdown]
# ## Comparison

# %%
plt.figure(figsize=(8,5))
plt.plot(cumul_eg, label='Epsilon-Greedy')
plt.plot(cumul_ucb, label='UCB1')
plt.plot(cumul_sm, label='Softmax')
plt.plot(cumul_ts, label='Thompson Sampling')
plt.xlabel('Step')
plt.ylabel('Avg Cumulative Reward')
plt.title('Algorithm Comparison')
plt.legend()
plt.grid()
plt.show()

# %% [markdown]
# **Summary:**
# - Thompson Sampling & UCB1 achieve highest rewards.
# - Epsilon-Greedy offers simplicity and tunability.
# - Softmax provides a balance between the two.
